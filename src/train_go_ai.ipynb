{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqnIOf1oB2r9"
      },
      "source": [
        "# Entraînement d'un réseau de neurones pour jouer au Go\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/auduvignac/deep_learning_go/blob/main/src/train_go_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGW-Bl5SB2r_"
      },
      "source": [
        "## Description\n",
        "\n",
        "- [https://www.lamsade.dauphine.fr/~cazenave/DeepLearningProject.html](https://www.lamsade.dauphine.fr/~cazenave/DeepLearningProject.html)  \n",
        "- L'objectif est d'entraîner un réseau pour jouer au jeu de Go.  \n",
        "- Afin de garantir une équité en termes de ressources d'entraînement, le nombre de paramètres des réseaux soumis doit être inférieur à 100 000.  \n",
        "- Le nombre maximal d'étudiants par équipe est de deux.  \n",
        "- Les données utilisées pour l'entraînement proviennent des parties auto-jouées du programme Katago Go.  \n",
        "- Le jeu de données d'entraînement contient un total de 1 000 000 de parties différentes.  \n",
        "- Les données d'entrée sont composées de 31 plans de taille 19x19 :  \n",
        "  - Couleur au trait  \n",
        "  - Échelles  \n",
        "  - État actuel sur deux plans  \n",
        "  - Deux états précédents sur plusieurs plans  \n",
        "- Les cibles de sortie sont :  \n",
        "  - **La politique** : un vecteur de taille 361 avec `1.0` pour le coup joué, `0.0` pour les autres coups.  \n",
        "  - **La valeur** : une valeur entre `0.0` et `1.0` fournie par la recherche d'arbre Monte-Carlo, représentant la probabilité de victoire de Blanc.\n",
        "\n",
        "- Le projet a été écrit et fonctionne sous Ubuntu 22.04.  \n",
        "- Il utilise TensorFlow 2.9 et Keras pour le réseau.  \n",
        "- Un exemple de réseau convolutionnel avec deux têtes est donné dans le fichier `golois.py` et est sauvegardé dans le fichier `test.h5`.  \n",
        "- Les réseaux que vous concevez et entraînez doivent également avoir les mêmes têtes de politique et de valeur et être sauvegardés au format `.h5`.  \n",
        "- Un exemple de réseau et un épisode d'entraînement sont fournis dans le fichier `golois.py`.  \n",
        "- Si vous souhaitez compiler la bibliothèque Golois, vous devez installer **Pybind11** et exécuter `compile.sh`.\n",
        "\n",
        "## Tournois\n",
        "\n",
        "- Toutes les deux semaines environ, j'organiserai un tournoi entre les réseaux que vous téléchargez.  \n",
        "- Chaque nom de réseau correspond aux noms des étudiants qui ont conçu et entraîné le réseau.  \n",
        "- Le modèle doit être sauvegardé au format **Keras h5**.  \n",
        "- Un tournoi en **round robin** sera organisé et les résultats seront envoyés par e-mail.  \n",
        "- Chaque réseau sera utilisé par un moteur **PUCT**, qui disposera de **2 secondes de temps CPU** par coup pour jouer dans le tournoi.\n",
        "\n",
        "## Exemple de réseau\n",
        "\n",
        "```python\n",
        "planes = 31\n",
        "moves = 361\n",
        "N = 10000\n",
        "epochs = 20\n",
        "batch = 128\n",
        "filters = 32\n",
        "input_data = np.random.randint(2, size=(N, 19, 19, planes))\n",
        "input_data = input_data.astype ('float32')\n",
        "policy = np.random.randint(moves, size=(N,))\n",
        "policy = keras.utils.to_categorical (policy)\n",
        "value = np.random.randint(2, size=(N,))\n",
        "value = value.astype ('float32')\n",
        "end = np.random.randint(2, size=(N, 19, 19, 2))\n",
        "end = end.astype ('float32')\n",
        "groups = np.zeros((N, 19, 19, 1))\n",
        "groups = groups.astype ('float32')\n",
        "\n",
        "input = keras.Input(shape=(19, 19, planes), name='board')\n",
        "x = layers.Conv2D(filters, 1, activation='relu', padding='same')(input)\n",
        "for i in range (5):\n",
        "  x = layers.Conv2D(filters, 3, activation='relu', padding='same')(x)\n",
        "policy_head = layers.Conv2D(1, 1, activation='relu', padding='same', use_bias = False, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "policy_head = layers.Flatten()(policy_head)\n",
        "policy_head = layers.Activation('softmax', name='policy')(policy_head)\n",
        "value_head = layers.Conv2D(1, 1, activation='relu', padding='same', use_bias = False, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "value_head = layers.Flatten()(value_head)\n",
        "value_head = layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(value_head)\n",
        "value_head = layers.Dense(1, activation='sigmoid', name='value', kernel_regularizer=regularizers.l2(0.0001))(value_head)\n",
        "model = keras.Model(inputs=input, outputs=[policy_head, value_head])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.0005, momentum=0.9),\n",
        "loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n",
        "loss_weights={'policy' : 1.0, 'value' : 1.0},\n",
        "metrics={'policy': 'categorical_accuracy', 'value': 'mse'})\n",
        "for i in range (1, epochs + 1):\n",
        "  print ('epoch ' + str (i))\n",
        "  golois.getBatch (input_data, policy, value, end, groups, i * N)\n",
        "  history = model.fit(input_data,\n",
        "  {'policy': policy, 'value': value},\n",
        "  epochs=1, batch_size=batch)\n",
        "  if (i % 5 == 0):\n",
        "  gc.collect ()\n",
        "  if (i % 20 == 0):\n",
        "  golois.getValidation (input_data, policy, value, end)\n",
        "  val = model.evaluate (input_data,\n",
        "  [policy, value], verbose = 0, batch_size=batch)\n",
        "  print (\"val =\", val)\n",
        "  model.save ('test.h5')\n",
        "```\n",
        "\n",
        "## Instructions :  \n",
        "- Entraînez un réseau pour jouer au Go.  \n",
        "- Soumettez les réseaux entraînés **avant samedi soir**.  \n",
        "- Tournoi des réseaux **chaque dimanche**.  \n",
        "- Téléchargez un réseau **avant la fin de la session**.\n",
        "\n",
        "## Mise en place de l'environnement de travail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VvTcnNjdcrGe",
        "outputId": "0e070fed-c4a7-485b-f070-1b115eb7235a"
      },
      "outputs": [],
      "source": [
        "!wget https://www.lamsade.dauphine.fr/~cazenave/project2025.zip\n",
        "!unzip project2025.zip\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LXj_yFKDbhiI",
        "outputId": "893b147b-b1b6-4faa-9a3e-355038727a76"
      },
      "outputs": [],
      "source": [
        "!pip install tensorrt-bindings==8.6.1\n",
        "!pip install --extra-index-url https://pypi.nvidia.com tensorrt-libs\n",
        "!pip install tensorflow[and-cuda]==2.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rappels Apprentissage par renforcement\n",
        "\n",
        "### Optimisation des récompenses et recherche de politique (agent et politique)\n",
        "\n",
        "« L’apprentissage par renforcement (en anglais, *reinforcement learning*, ou RL) est la branche de l’apprentissage automatique qui consiste à apprendre comment un agent doit se comporter dans un environnement de manière à maximiser une récompense. Naturellement, l’apprentissage par renforcement profond restreint la méthode d’apprentissage à l’apprentissage profond » ([Charniak, E. (2019). *Introduction au Deep Learning*. Dunod. p. 105](https://www.dunod.com/sciences-techniques/introduction-au-deep-learning)).\n",
        "\n",
        "« Dans l’apprentissage par renforcement, un agent logiciel procède à des observations et réalise des actions au sein d’un environnement. En retour, il reçoit des récompenses. Son objectif est d’apprendre à agir de façon à maximiser les récompenses espérées sur le long terme » ([Géron, A. (2023). *Deep Learning avec Keras et TensorFlow* (3e éd.). O'Reilly. p. 440](https://www.dunod.com/sciences-techniques/deep-learning-avec-keras-et-tensorflow-mise-en-oeuvre-et-cas-concrets-0)).\n",
        "\n",
        "« L’algorithme que l’agent logiciel utilise pour déterminer ses actions est appelé stratégie ou politique (*policy*). Cette politique peut être un réseau de neurones qui prend en entrée des observations et produit en sortie l’action à réaliser » ([Géron, A. (2023). *Deep Learning avec Keras et TensorFlow* (3e éd.). O'Reilly. p. 441](https://www.dunod.com/sciences-techniques/deep-learning-avec-keras-et-tensorflow-mise-en-oeuvre-et-cas-concrets-0)).\n",
        "\n",
        "\n",
        "Dans le cas du jeu de go: \n",
        "- L'agent est le programme qui joue au jeu ;\n",
        "- L'environnement est le plateau de jeu ;\n",
        "- Les récompenses sont les points gagnés ou perdus lors d'une partie ;\n",
        "- La politique définit la manière dont l’agent choisit ses coups en fonction de l’état du plateau, dans le but de maximiser ses gains à long terme.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "L’objectif de ce projet est de concevoir un réseau de neurones permettant de jouer au jeu de go. La figure ci-dessous (cf. [Pumperla, M., & Ferguson, K. (2019). *Deep Learning and the Game of Go*. Manning Publications. p.117](https://www.amazon.fr/Deep-Learning-Game-Max-Pumperla/dp/1617295329)) illustre comment les sorties du réseau, représentant les probabilités associées aux coups possibles, sont utilisées pour déterminer l’action optimale à effectuer.\n",
        "\n",
        "![Neural network go](https://raw.githubusercontent.com/auduvignac/deep_learning_go/refs/heads/main/figures/go_explanation_deep_annotated.png?token=GHSAT0AAAAAADA7LSSO5U4CC4HBJZQI5X2YZ7AGJBQ)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR4CT2R1qaB1"
      },
      "source": [
        "## Importation des librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAcwAJJ9qZF7"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from google.colab import files\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import gc # garbage collector\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import golois"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtixgy9JqeA7"
      },
      "source": [
        "## Création de la classe abstraite `GONet`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_6C3MiaVelc"
      },
      "outputs": [],
      "source": [
        "class GONet(ABC):\n",
        "\n",
        "    def __init__(\n",
        "        self, batch=128, epochs=20, filters=32, moves=361, N=10000, planes=31\n",
        "    ):\n",
        "        self.batch = batch\n",
        "        self.epochs = epochs\n",
        "        self.filters = filters\n",
        "        self.moves = moves\n",
        "        self.N = N\n",
        "        self.planes = planes\n",
        "        self.set_input_data(N, planes)\n",
        "        self.set_policy(N, moves)\n",
        "        self.set_value(N)\n",
        "        self.set_end(N)\n",
        "        self.set_groups(N)\n",
        "        print(\"Tensorflow version\", tf.__version__)\n",
        "        print(\"getValidation\", flush=True)\n",
        "        golois.getValidation(\n",
        "            self.input_data, self.policy, self.value, self.end\n",
        "        )\n",
        "        self.loss_total = []\n",
        "        self.policy_loss  = []\n",
        "        self.value_loss  = []\n",
        "        self.policy_acc = []\n",
        "        self.value_mse = []\n",
        "\n",
        "    def set_input_data(self, N, planes):\n",
        "        input_data = np.random.randint(2, size=(N, 19, 19, planes))\n",
        "        self.input_data = input_data.astype(\"float32\")\n",
        "\n",
        "    def get_input_data(self):\n",
        "        return self.input_data\n",
        "\n",
        "    def set_policy(self, N, moves):\n",
        "        policy = np.random.randint(moves, size=(N,))\n",
        "        self.policy = keras.utils.to_categorical(policy)\n",
        "\n",
        "    def get_policy(self):\n",
        "        return self.policy\n",
        "\n",
        "    def set_value(self, N):\n",
        "        value = np.random.randint(2, size=(N,))\n",
        "        self.value = value.astype(\"float32\")\n",
        "\n",
        "    def get_value(self):\n",
        "        return self.value\n",
        "\n",
        "    def set_end(self, N):\n",
        "        end = np.random.randint(2, size=(N, 19, 19, 2))\n",
        "        self.end = end.astype(\"float32\")\n",
        "\n",
        "    def get_end(self):\n",
        "        return self.end\n",
        "\n",
        "    def set_groups(self, N):\n",
        "        groups = np.zeros((N, 19, 19, 1))\n",
        "        self.groups = groups.astype(\"float32\")\n",
        "\n",
        "    def get_groups(self):\n",
        "        return self.groups\n",
        "\n",
        "    @abstractmethod\n",
        "    def set_model(self):\n",
        "        raise NotImplementedError(\"\"\"set_model() must be implemented in subclasses\")\"\"\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def create_policy_value_heads(self):\n",
        "        \"\"\"\n",
        "        Fonction de création des en-têtes de politique et de valeur.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"\"\"create_policy_value_heads() must be implemented in subclasses\"\"\")\n",
        "\n",
        "    def train_model(self, epochs):\n",
        "        for i in range(1, epochs + 1):\n",
        "            print(f\"epoch {str(i)}\")\n",
        "            golois.getBatch(\n",
        "                self.input_data,\n",
        "                self.policy,\n",
        "                self.value,\n",
        "                self.end,\n",
        "                self.groups,\n",
        "                i * self.N,\n",
        "            )\n",
        "            history = self.model.fit(\n",
        "                self.input_data,\n",
        "                {\"policy\": self.policy, \"value\": self.value},\n",
        "                epochs=1,\n",
        "                batch_size=self.batch,\n",
        "            )\n",
        "            # Extraction des valeurs depuis history.history\n",
        "            self.loss_total.append(history.history[\"loss\"][0])  # Loss globale\n",
        "            self.policy_loss.append(history.history[\"policy_loss\"][0])  # Policy loss\n",
        "            self.value_loss.append(history.history[\"value_loss\"][0])  # Value loss\n",
        "            self.policy_acc.append(history.history[\"policy_categorical_accuracy\"][0])  # Policy accuracy\n",
        "            self.value_mse.append(history.history[\"value_mse\"][0])  # Value MSE\n",
        "            if i % 5 == 0:\n",
        "                gc.collect()\n",
        "            if i % 20 == 0:\n",
        "                golois.getValidation(\n",
        "                    self.input_data, self.policy, self.value, self.end\n",
        "                )\n",
        "                val = self.model.evaluate(\n",
        "                    self.input_data,\n",
        "                    [self.policy, self.value],\n",
        "                    verbose=0,\n",
        "                    batch_size=self.batch,\n",
        "                )\n",
        "                print(f\"{val=}\")\n",
        "\n",
        "    def save_model(self, name):\n",
        "        self.model.save(name)\n",
        "        files.download(name)\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Trace les courbes des pertes et métriques en fonction des époques.\"\"\"\n",
        "        epochs = range(1, len(self.loss_total) + 1)  # Liste des époques\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Graphique des pertes\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, self.loss_total, label=\"Loss totale\", marker=\"o\")\n",
        "        plt.plot(epochs, self.policy_loss, label=\"Policy Loss\", marker=\"o\")\n",
        "        plt.plot(epochs, self.value_loss, label=\"Value Loss\", marker=\"o\")\n",
        "        plt.xlabel(\"Époques\")\n",
        "        plt.ylabel(\"Valeur de la perte\")\n",
        "        plt.title(\"Évolution des pertes\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Graphique des métriques\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, self.policy_acc, label=\"Policy Accuracy\", marker=\"o\")\n",
        "        plt.plot(epochs, self.value_mse, label=\"Value MSE\", marker=\"o\")\n",
        "        plt.xlabel(\"Époques\")\n",
        "        plt.ylabel(\"Valeur des métriques\")\n",
        "        plt.title(\"Évolution des métriques\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Affichage\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def log_accuracy(self, results_dict={}):\n",
        "        \"\"\"Logs the model's last accuracy into the given dictionary.\"\"\"\n",
        "        results_dict[self.__class__.__name__] = {\n",
        "          \"instance\": self,\n",
        "          \"accuracy\": self.policy_acc[-1]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXuo5huH7cIy"
      },
      "outputs": [],
      "source": [
        "def save_best_model(results_dict, model_name=\"test.h5\"):\n",
        "    \"\"\"\n",
        "    Returns the class instance with the highest accuracy from the dictionary.\n",
        "    \"\"\"\n",
        "    if not results_dict:\n",
        "        return None  # Handle empty dictionary case\n",
        "\n",
        "    # Find the key with the max accuracy\n",
        "    best_model_key = max(results_dict, key=lambda k: results_dict[k][\"accuracy\"])\n",
        "\n",
        "    # Retrieve the best accuracy\n",
        "    best_accuracy = results_dict[best_model_key][\"accuracy\"]\n",
        "\n",
        "    # Retrieve the best instance\n",
        "    best_instance = results_dict[best_model_key][\"instance\"]\n",
        "\n",
        "    print(f\"Le réseau {best_model_key} est celui qui a enregistré la meilleur accuracy : {best_accuracy}\")\n",
        "\n",
        "    best_instance.save_model(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous allons définir un dictionnaire intitulé `log_accuracy_dict` qui contiendra les *accuracies* successives pour chaque réseau. Ce dernier constituera un historique des performances des réseaux entraînés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-uT2eey8Ph0"
      },
      "outputs": [],
      "source": [
        "log_accuracy_dict = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNWs9l8L0bWx"
      },
      "source": [
        "Création de la classe `GONetDemo`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfuVC90KRFDQ"
      },
      "outputs": [],
      "source": [
        "class GONetDemo(GONet):\n",
        "\n",
        "    def __init__(\n",
        "        self, batch=128, epochs=20, filters=32, moves=361, N=10000, planes=31\n",
        "    ):\n",
        "        super().__init__(batch, epochs, filters, moves, N, planes)\n",
        "\n",
        "    def create_policy_value_heads(self, x, input_layer):\n",
        "        \"\"\"\n",
        "        Fonction de création des en-têtes de politique et de valeur.\n",
        "        \"\"\"\n",
        "        policy_head = layers.Conv2D(\n",
        "            1,\n",
        "            1,\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "            use_bias=False,\n",
        "            kernel_regularizer=regularizers.l2(0.0001),\n",
        "        )(x)\n",
        "        policy_head = layers.Flatten()(policy_head)\n",
        "        policy_head = layers.Activation(\"softmax\", name=\"policy\")(policy_head)\n",
        "        value_head = layers.Conv2D(\n",
        "            1,\n",
        "            1,\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "            use_bias=False,\n",
        "            kernel_regularizer=regularizers.l2(0.0001),\n",
        "        )(x)\n",
        "        value_head = layers.Flatten()(value_head)\n",
        "        value_head = layers.Dense(\n",
        "            50, activation=\"relu\", kernel_regularizer=regularizers.l2(0.0001)\n",
        "        )(value_head)\n",
        "        value_head = layers.Dense(\n",
        "            1,\n",
        "            activation=\"sigmoid\",\n",
        "            name=\"value\",\n",
        "            kernel_regularizer=regularizers.l2(0.0001),\n",
        "        )(value_head)\n",
        "\n",
        "        model = keras.Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.SGD(learning_rate=0.0005, momentum=0.9),\n",
        "            loss={\n",
        "                \"policy\": \"categorical_crossentropy\",\n",
        "                \"value\": \"binary_crossentropy\",\n",
        "            },\n",
        "            loss_weights={\"policy\": 1.0, \"value\": 1.0},\n",
        "            metrics={\"policy\": \"categorical_accuracy\", \"value\": \"mse\"},\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def set_model(self):\n",
        "        input_layer = keras.Input(shape=(19, 19, self.planes), name=\"board\")\n",
        "        x = layers.Conv2D(self.filters, 1, activation=\"relu\", padding=\"same\")(\n",
        "            input_layer\n",
        "        )\n",
        "        for _ in range(5):\n",
        "            x = layers.Conv2D(\n",
        "                self.filters, 3, activation=\"relu\", padding=\"same\"\n",
        "            )(x)\n",
        "        self.model = self.create_policy_value_heads(x, input_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CVZ_rb1QVelh",
        "outputId": "91a71821-394d-4f8d-93b5-be8deffe2578"
      },
      "outputs": [],
      "source": [
        "GONetDemo_instance = GONetDemo()\n",
        "GONetDemo_instance.set_model()\n",
        "GONetDemo_instance.train_model(20)\n",
        "GONetDemo_instance.plot_training_history()\n",
        "GONetDemo_instance.log_accuracy(log_accuracy_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEV7ZUUU0g-3"
      },
      "source": [
        "Création de la classe `GONetRes_cnn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSmJCGWqpZ8F"
      },
      "outputs": [],
      "source": [
        "class GONetRes_cnn(GONet):\n",
        "    def __init__(\n",
        "        self, batch=128, epochs=20, filters=32, moves=361, N=10000, planes=31\n",
        "    ):\n",
        "        super().__init__(batch, epochs, filters, moves, N, planes)\n",
        "\n",
        "    def create_policy_value_heads(self, x, input_layer):\n",
        "        \"\"\"\n",
        "        Fonction de création des en-têtes de politique et de valeur.\n",
        "        \"\"\"\n",
        "        policy_head = layers.Conv2D(1, 1, activation='relu', padding='same', use_bias=False)(x)\n",
        "        policy_head = layers.Flatten()(policy_head)\n",
        "        policy_head = layers.Activation('softmax', name='policy')(policy_head)\n",
        "\n",
        "        value_head = layers.Conv2D(1, 1, activation='relu', padding='same', use_bias=False)(x)\n",
        "        value_head = layers.Flatten()(value_head)\n",
        "        value_head = layers.Dense(50, activation='relu')(value_head)\n",
        "        value_head = layers.Dense(1, activation='sigmoid', name='value')(value_head)\n",
        "\n",
        "        model = models.Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "                      loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n",
        "                      metrics={'policy': 'categorical_accuracy', 'value': 'mse'})\n",
        "        return model\n",
        "\n",
        "    def set_model(self, filters=32, planes=31):\n",
        "        input_layer = layers.Input(shape=(19, 19, planes), name='board')\n",
        "        x = layers.Conv2D(filters, 1, activation='relu', padding='same')(input_layer)\n",
        "        for _ in range(5):\n",
        "            x = layers.Conv2D(filters, 3, activation='relu', padding='same')(x)\n",
        "        self.model = self.create_policy_value_heads(x, input_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y720WYUsq7--",
        "outputId": "e44cd560-bfe4-4226-c039-8184a259149d"
      },
      "outputs": [],
      "source": [
        "GONetRes_cnn_instance = GONetRes_cnn()\n",
        "GONetRes_cnn_instance.set_model()\n",
        "GONetRes_cnn_instance.train_model(20)\n",
        "GONetRes_cnn_instance.plot_training_history()\n",
        "GONetRes_cnn_instance.log_accuracy(log_accuracy_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t71Qs2Rg0lS6"
      },
      "source": [
        "## Création de la classe `GONetRes_resnet`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iofDxz7OsGh_"
      },
      "outputs": [],
      "source": [
        "class GONetRes_resnet(GONet):\n",
        "    def __init__(\n",
        "        self, batch=128, epochs=20, filters=32, moves=361, N=10000, planes=31\n",
        "    ):\n",
        "        super().__init__(batch, epochs, filters, moves, N, planes)\n",
        "\n",
        "    def create_policy_value_heads(self, x, input_layer):\n",
        "        \"\"\"\n",
        "        Fonction de création des en-têtes de politique et de valeur.\n",
        "        \"\"\"\n",
        "        policy_head = layers.Conv2D(1, 1, activation='relu', padding='same', use_bias=False)(x)\n",
        "        policy_head = layers.Flatten()(policy_head)\n",
        "        policy_head = layers.Activation('softmax', name='policy')(policy_head)\n",
        "\n",
        "        value_head = layers.Conv2D(1, 1, activation='relu', padding='same', use_bias=False)(x)\n",
        "        value_head = layers.Flatten()(value_head)\n",
        "        value_head = layers.Dense(50, activation='relu')(value_head)\n",
        "        value_head = layers.Dense(1, activation='sigmoid', name='value')(value_head)\n",
        "\n",
        "        model = models.Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "                      loss={'policy': 'categorical_crossentropy', 'value': 'binary_crossentropy'},\n",
        "                      metrics={'policy': 'categorical_accuracy', 'value': 'mse'})\n",
        "        return model\n",
        "\n",
        "    def residual_block(self, x, filters):\n",
        "        shortcut = x\n",
        "        x = layers.Conv2D(filters, (3,3), padding='same', use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU()(x)\n",
        "        x = layers.Conv2D(filters, (3,3), padding='same', use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Add()([x, shortcut])\n",
        "        x = layers.ReLU()(x)\n",
        "        return x\n",
        "\n",
        "    def set_model(self, filters=64, planes=31):\n",
        "        input_layer = layers.Input(shape=(19, 19, planes), name='board')\n",
        "        x = layers.Conv2D(filters, 3, activation='relu', padding='same')(input_layer)\n",
        "        for _ in range(5):\n",
        "            x = self.residual_block(x, filters)\n",
        "        self.model = self.create_policy_value_heads(x, input_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bwkWX5hfs2s6",
        "outputId": "80a31476-61a3-4176-b067-53154d14c857"
      },
      "outputs": [],
      "source": [
        "GONetRes_resnet_instance = GONetRes_resnet()\n",
        "GONetRes_resnet_instance.set_model()\n",
        "GONetRes_resnet_instance.train_model(20)\n",
        "GONetRes_resnet_instance.plot_training_history()\n",
        "GONetRes_resnet_instance.log_accuracy(log_accuracy_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "1zImqELY55Zh",
        "outputId": "ed575033-8004-40a1-fd13-32b470a1ea55"
      },
      "outputs": [],
      "source": [
        "save_best_model(log_accuracy_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Compare Resnets, Mobilenets and Convnexts, Shufflenet by training on the game of go**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# *Shufflenet*\n",
        "\n",
        "*ShuffleNet* est une architecture de réseau neuronal conçue pour être rapide et efficace. Elle repose sur le concept de permutation des canaux du tenseur d’entrée, ce qui améliore l’efficacité en termes de calcul et d’utilisation de la mémoire.\n",
        "\n",
        "Le réseau se compose de deux principales parties :\n",
        "\n",
        "1.  Couche de convolution : Cette couche a pour rôle d’extraire les caractéristiques du tenseur d’entrée.\n",
        "\n",
        "2.  Couche de permutation (*shuffling*) : Cette couche permutent les canaux du tenseur d’entrée. Elle est conçue pour être légère et efficace, ce qui contribue fortement à la performance globale et à l’efficacité du réseau\n",
        "\n",
        "*Shufflenet* est un mobilenet avec moins de paramètres puisqu'il y a des plans séparés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Conv2D,\n",
        "    DepthwiseConv2D,\n",
        "    Dense,\n",
        "    Concatenate,\n",
        "    Add,\n",
        "    ReLU,\n",
        "    BatchNormalization,\n",
        "    AvgPool2D,\n",
        "    MaxPool2D,\n",
        "    GlobalAveragePooling2D,\n",
        "    Reshape,\n",
        "    Permute,\n",
        "    Lambda,\n",
        "    Flatten,\n",
        "    Activation,\n",
        ")\n",
        "\n",
        "\n",
        "class GONetRes_shufflenet(GONet):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch=128,\n",
        "        epochs=20,\n",
        "        filters=32,\n",
        "        moves=361,\n",
        "        N=10000,\n",
        "        planes=31,\n",
        "        trunk=32,  # trunk (hyperparamètre) : nombre de filtres dans le tronc (32, 64, etc.). Dans le modèle de go il est de l'ordre de 32.\n",
        "        blocks=5,  # blocks : nombre de blocs résiduels\n",
        "    ):\n",
        "        super().__init__(batch, epochs, filters, moves, N, planes)\n",
        "        self.trunk = trunk\n",
        "        self.blocks = blocks\n",
        "\n",
        "    def create_policy_value_heads(self, x, input_layer):\n",
        "        input = keras.Input(shape=(19, 19, self.planes), name=\"board\")\n",
        "        x = Conv2D(\n",
        "            self.trunk,\n",
        "            1,\n",
        "            padding=\"same\",\n",
        "            kernel_regularizer=regularizers.l2(0.0001),\n",
        "        )(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = ReLU()(x)\n",
        "        for i in range(self.blocks):\n",
        "            x = self.bottleneck_block(x, self.filters, self.trunk)\n",
        "        policy_head = Conv2D(\n",
        "            1,\n",
        "            1,\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "            use_bias=False,\n",
        "            kernel_regularizer=regularizers.l2(0.0001),\n",
        "        )(x)\n",
        "        policy_head = Flatten()(policy_head)\n",
        "        policy_head = Activation(\"softmax\", name=\"policy\")(policy_head)\n",
        "        value_head = GlobalAveragePooling2D()(x)\n",
        "        value_head = Dense(\n",
        "            50, activation=\"relu\", kernel_regularizer=regularizers.l2(0.0001)\n",
        "        )(value_head)\n",
        "        value_head = Dense(\n",
        "            1,\n",
        "            activation=\"sigmoid\",\n",
        "            name=\"value\",\n",
        "            kernel_regularizer=regularizers.l2(0.0001),\n",
        "        )(value_head)\n",
        "        model = keras.Model(inputs=input, outputs=[policy_head, value_head])\n",
        "        return model\n",
        "\n",
        "    def bottleneck_block(self, tensor, expand=96, squeeze=16):\n",
        "        x = self.gconv(tensor, channels=expand, groups=4)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = ReLU()(x)\n",
        "        x = self.channel_shuffle(x, groups=4)\n",
        "        # Depthwise (comme dans les mobilenets)\n",
        "        x = DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = self.gconv(x, channels=squeeze, groups=4)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Add()([tensor, x])\n",
        "        # Les connexions résiduelles ajoutent de la souplesse et permettre de\n",
        "        # rendre les réseaux plus entraînables\n",
        "        output = ReLU()(x)\n",
        "        return output\n",
        "\n",
        "    def gconv(tensor, channels, groups):\n",
        "        \"\"\"\n",
        "        gconv permet de faire un groupe de convolution sur l'entrée du tenseur\n",
        "\n",
        "        Args:\n",
        "            tensor (_type_): _description_\n",
        "            channels (_type_): _description_\n",
        "            groups (_type_): _description_\n",
        "\n",
        "        Returns:\n",
        "            _type_: _description_\n",
        "        \"\"\"\n",
        "        input_ch = tensor.get_shape().as_list()[-1]\n",
        "        group_ch = input_ch // groups\n",
        "        output_ch = channels // groups\n",
        "        groups_list = []\n",
        "        for i in range(groups):\n",
        "            group_tensor = tensor[:, :, :, i * group_ch : (i + 1) * group_ch]\n",
        "            group_tensor = Conv2D(output_ch, 1)(group_tensor)\n",
        "            groups_list.append(group_tensor)\n",
        "        output = Concatenate()(groups_list)\n",
        "        return output\n",
        "\n",
        "    def channel_shuffle(self, x, groups):\n",
        "        _, width, height, channels = x.get_shape().as_list()\n",
        "        group_ch = channels // groups\n",
        "        x = Reshape([width, height, group_ch, groups])(x)\n",
        "        x = Permute([1, 2, 4, 3])(x)\n",
        "        x = Reshape([width, height, channels])(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Glossaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Réseau de neurones *feed forward* :\n",
        "\n",
        "« Un réseau de neurones à propagation avant, en anglais *feed forward neural network*, est un réseau de neurones artificiels acyclique, se distinguant ainsi des réseaux de neurones récurrents. Le plus connu est le perceptron multicouche qui est une extension du premier réseau de neurones artificiel. Le perceptron a été inventé en 1957 par Frank Rosenblatt.\n",
        "\n",
        "Le réseau de neurones à propagation avant a été le premier et le plus simple type de réseau neuronal artificiel conçu. Typiquement, il ne comportait qu'une seule couche cachée et s'appelait perceptron. Dans ce réseau, l'information ne se déplace que dans une seule direction, vers l'avant, à partir des nœuds d'entrée, en passant par les couches cachées (le cas échéant) et vers les nœuds de sortie. Il n'y a pas de cycles ou de boucles dans ce réseau. C'est pourquoi on le désigne parfois par réseau de neurones sans boucle.\n",
        "\n",
        "Quand le réseau de neurones à propagation avant comporte plusieurs couches cachées, on parle habituellement d'un perceptron multicouche. » (cf. [Réseau de neurones *feed forward*](https://datafranca.org/wiki/R%C3%A9seau_de_neurones_%C3%A0_propagation_avant))\n",
        "\n",
        "\n",
        "\n",
        "![Réseau de neurones *feed forward*](https://upload.wikimedia.org/wikipedia/commons/8/82/FeedForwardNN.png)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
